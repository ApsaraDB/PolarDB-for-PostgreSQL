/*-------------------------------------------------------------------------
 *
 * polar_ringbuf.c
 *	  PolarDB ring buffer interface routines.
 *
 * Copyright (c) 2022, Alibaba Group Holding Limited
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 * IDENTIFICATION
 *	  src/backend/access/logindex/polar_ringbuf.c
 *
 *-------------------------------------------------------------------------
 */

/*
 * Principal entry points:
 *
 * polar_ringbuf_pkt_reserve() -- Reserve space from ring buffer for future write.
 *          And this function should be protected by exclusive lock.
 *
 * polar_ringbuf_pkt_write() -- Write packet data to reserved space and it's safe for multi process
 *          to write different packet
 *
 * polar_ringbuf_new_ref() -- Create new ring buffer reference
 *
 * polar_ringbuf_read_next_pkt() -- Read data from ring buffer sequentially
 *
 */

#include "postgres.h"

#include "access/polar_logindex_redo.h"
#include "access/polar_ringbuf.h"
#include "miscadmin.h"
#include "pgstat.h"
#include "storage/ipc.h"
#include "utils/polar_bitpos.h"

static bool polar_ringbuf_update_pread(polar_ringbuf_t rbuf);

#define RINGBUF_REF_MATCH(ref) \
	((ref)->slot >= 0 && (ref)->slot < POLAR_RINGBUF_MAX_SLOT \
	 && (ref)->rbuf->slot[(ref)->slot].ref_num == (ref)->ref_num)

/*
 * Initialize ring buffer from allocated memory.
 */
polar_ringbuf_t
polar_ringbuf_init(uint8 *data, size_t len, int tranche_id)
{
	polar_ringbuf_t rbuf = (polar_ringbuf_t) data;

	LWLockInitialize(&rbuf->lock, tranche_id);

	pg_atomic_init_u64(&rbuf->pread, 0);
	pg_atomic_init_u64(&rbuf->pwrite, 0);

	pg_atomic_init_u64(&rbuf->prs.push_cnt, 0);
	pg_atomic_init_u64(&rbuf->prs.free_up_cnt, 0);
	pg_atomic_init_u64(&rbuf->prs.send_phys_io_cnt, 0);
	pg_atomic_init_u64(&rbuf->prs.total_written, 0);
	pg_atomic_init_u64(&rbuf->prs.total_read, 0);
	pg_atomic_init_u64(&rbuf->prs.prev_pop_cnt, 0);
	pg_atomic_init_u64(&rbuf->prs.evict_ref_cnt, 0);

	MemSet(rbuf->slot, 0, sizeof(polar_ringbuf_slot_t) * POLAR_RINGBUF_MAX_SLOT);

	rbuf->size = len - offsetof(polar_ringbuf_data_t, data);
	rbuf->occupied = 0;
	return rbuf;
}

/*
 * Create new reference for ring buffer and return true on success
 */
bool
polar_ringbuf_new_ref(polar_ringbuf_t rbuf, bool strong,
					  polar_ringbuf_ref_t *ref, char *ref_name)
{
	int			i;
	uint64		unoccupied;
	bool		succeed = false;

	elog(LOG, "create reference for %s", ref_name);

	LWLockAcquire(&rbuf->lock, LW_EXCLUSIVE);
	unoccupied = ~(rbuf->occupied);
	POLAR_BIT_LEAST_POS(unoccupied, i);

	/* The position start from 1 */
	if (i > 0 && i <= POLAR_RINGBUF_MAX_SLOT)
	{
		POLAR_BIT_OCCUPY(rbuf->occupied, i);
		i--;
		rbuf->slot[i].strong = strong;
		rbuf->slot[i].pread = pg_atomic_read_u64(&rbuf->pread);
		rbuf->slot[i].visit = rbuf->min_visit;
		rbuf->slot[i].ref_num = ++rbuf->ref_num;
		ref->rbuf = rbuf;
		ref->ref_num = rbuf->slot[i].ref_num;
		ref->slot = i;
		ref->strong = strong;

		if (ref_name != NULL)
		{
			strncpy(rbuf->slot[i].ref_name, ref_name, POLAR_RINGBUF_MAX_REF_NAME);
			rbuf->slot[i].ref_name[POLAR_RINGBUF_MAX_REF_NAME] = '\0';
		}
		else
			rbuf->slot[i].ref_name[0] = '\0';

		strcpy(ref->ref_name, rbuf->slot[i].ref_name);

		succeed = true;
	}

	LWLockRelease(&rbuf->lock);

	return succeed;
}

/*
 * release ring buffer reference
 */
void
polar_ringbuf_release_ref(polar_ringbuf_ref_t *ref)
{
	polar_ringbuf_t rbuf = ref->rbuf;

	if (rbuf == NULL)
		return;

	LWLockAcquire(&rbuf->lock, LW_EXCLUSIVE);

	if (RINGBUF_REF_MATCH(ref))
	{
		POLAR_BIT_RELEASE_OCCUPIED(rbuf->occupied, ref->slot + 1);
		MemSet(&rbuf->slot[ref->slot], 0, sizeof(polar_ringbuf_slot_t));
		polar_ringbuf_update_pread(rbuf);
	}

	LWLockRelease(&rbuf->lock);

	ref->rbuf = NULL;
}

/*
 * The callback function to do auto release reference
 */
static void
polar_ringbuf_auto_release(int code, Datum arg)
{
	polar_ringbuf_ref_t *ref = (polar_ringbuf_ref_t *) arg;

	polar_ringbuf_release_ref(ref);
}

/*
 * Register auto release for the reference
 * It will be released automatically when proc_exit
 */
void
polar_ringbuf_auto_release_ref(polar_ringbuf_ref_t *ref)
{
	/* check whether it is able to register a before_shmem_exit callback */
	if (polar_check_before_shmem_exit(polar_ringbuf_auto_release, (Datum) ref, false))
		before_shmem_exit(polar_ringbuf_auto_release, (Datum) ref);
}

/*
 * Get strong reference from weak reference
 */
bool
polar_ringbuf_get_ref(polar_ringbuf_ref_t *ref)
{
	bool		got = false;
	polar_ringbuf_t rbuf = ref->rbuf;

	/*
	 * The strong reference property is changed in the same process, and it's
	 * safe to use shared lock
	 */
	LWLockAcquire(&rbuf->lock, LW_SHARED);

	if (RINGBUF_REF_MATCH(ref))
	{
		rbuf->slot[ref->slot].strong = true;
		got = true;
	}

	LWLockRelease(&rbuf->lock);

	return got;
}

/*
 * Check whether it's valid reference
 */
bool
polar_ringbuf_valid_ref(polar_ringbuf_ref_t *ref)
{
	bool		ret = false;

	if (!ref || !ref->rbuf)
		return ret;

	LWLockAcquire(&(ref->rbuf->lock), LW_SHARED);

	ret = RINGBUF_REF_MATCH(ref);

	LWLockRelease(&(ref->rbuf->lock));

	return ret;
}

/*
 * Change strong reference to weak reference
 */
bool
polar_ringbuf_clear_ref(polar_ringbuf_ref_t *ref)
{
	bool		cleared = false;
	polar_ringbuf_t rbuf = ref->rbuf;

	/*
	 * The strong reference property is changed in the same process, and it's
	 * safe to use shared lock
	 */
	LWLockAcquire(&rbuf->lock, LW_SHARED);

	if (RINGBUF_REF_MATCH(ref))
	{
		rbuf->slot[ref->slot].strong = false;
		cleared = true;
	}

	LWLockRelease(&rbuf->lock);

	return cleared;
}

/*
 * The reference read packet sequentially.
 * Support read from the offset of packet
 */
ssize_t
polar_ringbuf_read_next_pkt(polar_ringbuf_ref_t *ref,
							int offset, uint8 *buf, size_t len)
{
	size_t		todo;
	size_t		split;
	size_t		pktlen;
	polar_ringbuf_t rbuf = ref->rbuf;
	size_t		idx = rbuf->slot[ref->slot].pread;

	pktlen = polar_ringbuf_pkt_len(rbuf, idx);

	if (unlikely(offset >= pktlen))
		return -1;

	if ((offset + len) > pktlen)
		len = pktlen - offset;

	idx = (idx + POLAR_RINGBUF_PKTHDRSIZE + offset) % rbuf->size;
	todo = len;
	split = ((idx + len) > rbuf->size) ? rbuf->size - idx : 0;

	if (unlikely(split > 0))
	{
		memcpy(buf, rbuf->data + idx, split);
		buf += split;
		todo -= split;
		idx = 0;
	}

	memcpy(buf, rbuf->data + idx, todo);

	return len;
}

/*
 * Write packet data and the idx is the start position of the packet.
 * Support write from the offset of packet.
 */
ssize_t
polar_ringbuf_pkt_write(polar_ringbuf_t rbuf, size_t idx, int offset, uint8 *buf, size_t len)
{
	size_t		todo;
	size_t		split;
	size_t		pktlen = polar_ringbuf_pkt_len(rbuf, idx);

	if (offset >= pktlen || (offset + len) > pktlen)
		return -1;

	idx = (idx + POLAR_RINGBUF_PKTHDRSIZE + offset) % rbuf->size;
	split = (idx + len > rbuf->size) ? rbuf->size - idx : 0;
	todo = len;

	if (split > 0)
	{
		memcpy(rbuf->data + idx, buf, split);
		buf += split;
		todo -= split;
		idx = 0;
	}

	memcpy(rbuf->data + idx, buf, todo);

	return len;
}

/*
 * Update the ring buffer least read position and to get more free space
 */
bool
polar_ringbuf_update_pread(polar_ringbuf_t rbuf)
{
	int			i;
	uint64		min_slot = POLAR_RINGBUF_MAX_SLOT;
	uint64		min_visit = UINT64_MAX;
	bool		updated = false;
	uint64		occupied = rbuf->occupied;

	while (occupied)
	{
		POLAR_BIT_LEAST_POS(occupied, i);
		i--;

		if (min_visit > rbuf->slot[i].visit)
		{
			min_visit = rbuf->slot[i].visit;
			min_slot = i;
		}

		POLAR_BIT_CLEAR_LEAST(occupied);
	}

	if (min_slot != POLAR_RINGBUF_MAX_SLOT && rbuf->min_visit != min_visit)
	{
		uint64		pre_pread = pg_atomic_read_u64(&rbuf->pread);

		rbuf->min_visit = min_visit;
		pg_atomic_write_u64(&rbuf->pread, rbuf->slot[min_slot].pread);
		updated = true;

		if (rbuf->slot[min_slot].pread <= pre_pread)
			pg_atomic_fetch_add_u64(&rbuf->prs.total_read, (uint64) (rbuf->size - (pre_pread - rbuf->slot[min_slot].pread)));
		else
			pg_atomic_fetch_add_u64(&rbuf->prs.total_read, (uint64) (rbuf->slot[min_slot].pread - pre_pread));
	}

	return updated;
}

/*
 * Update the reference's read position
 */
void
polar_ringbuf_update_ref(polar_ringbuf_ref_t *ref)
{
	polar_ringbuf_t rbuf = ref->rbuf;
	size_t		idx = rbuf->slot[ref->slot].pread;
	size_t		pktlen = polar_ringbuf_pkt_len(rbuf, idx);

	LWLockAcquire(&rbuf->lock, LW_SHARED);
	rbuf->slot[ref->slot].pread = (idx + POLAR_RINGBUF_PKTHDRSIZE + pktlen) % rbuf->size;
	rbuf->slot[ref->slot].visit++;
	LWLockRelease(&rbuf->lock);
}

/*
 * No enouth space to write and eliminate one weak reference with least read position
 */
static bool
polar_ringbuf_evict_ref(polar_ringbuf_t rbuf)
{
	int			i;
	bool		evicted = false;
	uint8		min_slot = POLAR_RINGBUF_MAX_SLOT;
	uint64		min_visit = UINT64_MAX;
	uint64		occupied = rbuf->occupied;

	while (occupied)
	{
		POLAR_BIT_LEAST_POS(occupied, i);
		i--;

		if (!rbuf->slot[i].strong
			&& min_visit > rbuf->slot[i].visit)
		{
			min_visit = rbuf->slot[i].visit;
			min_slot = i;
		}

		POLAR_BIT_CLEAR_LEAST(occupied);
	}

	if (min_slot != POLAR_RINGBUF_MAX_SLOT
		&& pg_atomic_read_u64(&rbuf->pread) == rbuf->slot[min_slot].pread)
	{
		elog(LOG, "polar evict slot=%d,name=%s which is weak reference", min_slot, rbuf->slot[min_slot].ref_name);
		POLAR_BIT_RELEASE_OCCUPIED(rbuf->occupied, min_slot + 1);
		MemSet(&rbuf->slot[min_slot], 0, sizeof(polar_ringbuf_slot_t));
		evicted = true;
		pg_atomic_fetch_add_u64(&rbuf->prs.evict_ref_cnt, 1);
	}

	return evicted;
}

/*
 * Update the least read position of the ring buffer to get more space for write
 */
void
polar_ringbuf_update_keep_data(polar_ringbuf_t rbuf)
{
	LWLockAcquire(&rbuf->lock, LW_EXCLUSIVE);
	polar_ringbuf_update_pread(rbuf);
	LWLockRelease(&rbuf->lock);
}

/*
 * Free up space until free size larger than aimed length
 */
void
polar_ringbuf_free_up(polar_ringbuf_t rbuf, size_t len, polar_interrupt_callback callback)
{
	bool		free_up = false;

	pgstat_report_wait_start(WAIT_EVENT_LOGINDEX_QUEUE_SPACE);

	pg_atomic_fetch_add_u64(&rbuf->prs.free_up_cnt, 1);

	do
	{
		LWLockAcquire(&rbuf->lock, LW_EXCLUSIVE);
		free_up = (polar_ringbuf_free_size(rbuf) > len);

		if (!free_up)
		{
			if (!polar_ringbuf_update_pread(rbuf)
				&& !polar_ringbuf_evict_ref(rbuf))
			{
				/*
				 * 1. Try to update reference's pread to get more space 2. Try
				 * to evict weak reference 3. Otherwise wait for reference to
				 * comsume data
				 */
				LWLockRelease(&rbuf->lock);
				CHECK_FOR_INTERRUPTS();

				if (callback != NULL)
					callback(rbuf);

				pg_usleep(10);
				continue;
			}
		}

		LWLockRelease(&rbuf->lock);
	}
	while (!free_up);

	pgstat_report_wait_end();
}

void
polar_ringbuf_ref_keep_data(polar_ringbuf_ref_t *ref, float ratio)
{
	polar_ringbuf_t rbuf = ref->rbuf;
	ssize_t		keep_size = rbuf->size * ratio;
	bool		updated = false;

	while (polar_ringbuf_avail(ref) > keep_size)
	{
		polar_ringbuf_update_ref(ref);
		updated = true;
	}

	if (updated)
		polar_ringbuf_update_keep_data(rbuf);
}

void
polar_ringbuf_reset(polar_ringbuf_t rbuf)
{
	for (;;)
	{
		LWLockAcquire(&rbuf->lock, LW_EXCLUSIVE);

		if (rbuf->occupied != 0)
		{
			LWLockRelease(&rbuf->lock);
			continue;
		}

		pg_atomic_write_u64(&rbuf->pread, 0);
		pg_atomic_write_u64(&rbuf->pwrite, 0);
		LWLockRelease(&rbuf->lock);
		break;
	}
}

void
polar_prs_stat_reset(polar_ringbuf_t queue)
{
	uint64		min_visit;

	if (!queue)
		return;

	LWLockAcquire(&queue->lock, LW_EXCLUSIVE);
	min_visit = queue->min_visit;
	LWLockRelease(&queue->lock);

	pg_atomic_write_u64(&queue->prs.push_cnt, 0);
	pg_atomic_write_u64(&queue->prs.prev_pop_cnt, min_visit);
	pg_atomic_write_u64(&queue->prs.free_up_cnt, 0);
	pg_atomic_write_u64(&queue->prs.send_phys_io_cnt, 0);
	pg_atomic_write_u64(&queue->prs.total_written, 0);
	pg_atomic_write_u64(&queue->prs.total_read, 0);
	pg_atomic_write_u64(&queue->prs.evict_ref_cnt, 0);
}
